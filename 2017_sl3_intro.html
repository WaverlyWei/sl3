<!DOCTYPE html>
<html>
  <head>
    <title>The sl3 R Package</title>
    <meta charset="utf-8">
    <meta name="author" content="Jeremy Coyle &amp; Nima Hejazi" />
    <link href="libs/remark-css-0.0.1/example.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# The <code>sl3</code> R Package
## A fresh approach to Super Learning
### <a href="">Jeremy Coyle</a> &amp; <a href="http://nimahejazi.org">Nima Hejazi</a>
### 2017-08-13 23:52:24

---




# Accessing these slides

--

### View them online:

* [stat.berkeley.edu/~nhejazi/present/2017_sl3_intro/2017_sl3_intro.html](https://www.stat.berkeley.edu/~nhejazi/present/2017_sl3_intro/2017_sl3_intro.html)

* here's a shortened URL: [https://goo.gl/bqP2tD](https://goo.gl/bqP2tD)

--

### View them locally:

_You'll need to clone the project branch containing the source materials for
this talk from GitHub. This includes the finalized slide deck (in HTML)._


```bash
git clone -b talk https://github.com/jeremyrcoyle/sl3.git
```

???

- This talk will focus on introducing the new `sl3` R package, which provides a
modern implementation of the Super Learner algorithm [@vdl2007super], a method
for performing stacked regressions [@breiman1996stacked] and combining this with
covariate screening and cross-validation.

---
class: inverse, center, middle

# `sl3`: The Core Design

---

# `sl3` Architecture

All of the classes defined in `sl3` are based on the R6 framework, which brings
an object-oriented paradigm to the R language.

## Core classes

- `task` - an object containing the data set of interest and specifying
important features of the data (e.g., repeated measures).

--

- `lrnr_base` - a base class that serves as the foundation for other classes
defined within `sl3`.

--

- `Pipeline` - a learner-type object created by chaining together screening and
learning algorithms, allowing for covariate screening and model fitting to be
subjected to the same cross-validation process.

--

- `Stack` - a stacked regression model -- that is, a learner-type object created
by combining different (independent) learners or pipelines.

???

- Probably good to point out that cross-validating a `Stack` allows for an
included `Pipeline` to be subjected to CV in the same way as other learners
(seriously awesome feature).

---
class: inverse, center, middle

# `sl3` in Action

## There ain't nothin' like a test drive...

---

# Get the package

- Currently, installation from the `master` branch is the only option:


```r
if (!("sl3" %in% installed.packages())) {
  devtools::install_github("jeremyrcoyle/sl3")
}
```



--

- Work is underway to set up a [`drat`
archive](http://dirk.eddelbuettel.com/code/drat.html) to host all stable
versions of the package.

--

- Of course, the package will eventually be available on CRAN, though a
submission to this repository is still far down the road.

--

To start using `sl3`, let's load the package:


```r
library(sl3)
```

---

# A "toy" data set

We can't do any model fitting without data. We'll use the `cpp` data set,
included with the package, for our experiment.


```r
# load example data set
data(cpp)
cpp &lt;- cpp %&gt;%
  dplyr::filter(!is.na(haz)) %&gt;%
  mutate_all(funs(replace(., is.na(.), 0)))

# here are the covariates we are interested in, and the outcome of course
covars &lt;- c("apgar1", "apgar5", "parity", "gagebrth", "mage", "meducyrs",
            "sexn")
outcome &lt;- "haz"
```

???

- A working implementation of a targeted learning approach to biomarker
discovery using moderated statistics.

- Next, we'll walk through analyzing some data.

---

# Setting up a `Task`

Recall that a `Task` is the core structure that holds the data set and
specifications to be respected in the estimation/modeling procedure.


```r
task &lt;- sl3_Task$new(cpp, covariates = covars, outcome = outcome)
```

--

Data is in the nodes:


```r
task$nodes
```

```
## $covariates
## [1] "apgar1"   "apgar5"   "parity"   "gagebrth" "mage"     "meducyrs"
## [7] "sexn"    
## 
## $outcome
## [1] "haz"
## 
## $id
## NULL
## 
## $weights
## NULL
```

???

- talk about tasks

---

# Screeners, learners, pipelines

Both screeners and learners can be easily instantiated by calling their `new`
methods:


```r
# a GLM learning algorithm, comes built-in with sl3
glm_learner &lt;- Lrnr_glm$new()
```

--

We can even pull screening and learning algorithms directly from
`Super Learner`:


```r
# an elastic net screener and learner, accessed via the Super Learner package
slscreener &lt;- Lrnr_pkg_SuperLearner_screener$new("screen.glmnet")
SL.glmnet_learner &lt;- Lrnr_pkg_SuperLearner$new(SL_wrapper = "SL.glmnet")
```

--

And pipelines can be easily created by stringing screeners and learners
together:


```r
# put them together in a pipeline
screen_and_glm &lt;- Pipeline$new(slscreener, glm_learner)
```

--

Training a learners (or pipelines) is as easy as calling the `train` method:


```r
# train the pipeline on the data in the task object
sg_fit &lt;- screen_and_glm$train(task)
```

---

# Stacks

We can create stacked regression models (Super Learners) by instantiating a new
stack with several existing learners. (Note that we can even incldue pipelines
in the model stack).


```r
# stacking learners/pipelines is essentially trivial
learner_stack &lt;- Stack$new(SL.glmnet_learner, glm_learner, screen_and_glm)

# train the stacked regression model via its built-in train method
stack_fit &lt;- learner_stack$train(task)
```

--

Let's take a look at the predictions made by the stacked model:


```r
preds &lt;- stack_fit$predict()
head(preds)
```

```
##      Lrnr_pkg_SuperLearner_SL.glmnet   Lrnr_glm
## [1,]                      0.35345519 0.36298498
## [2,]                      0.35345519 0.36298498
## [3,]                      0.24554305 0.25993072
## [4,]                      0.24554305 0.25993072
## [5,]                      0.24554305 0.25993072
## [6,]                      0.02953193 0.05680264
##      Lrnr_pkg_SuperLearner_screener_screen.glmnet___Lrnr_glm
## [1,]                                              0.36228209
## [2,]                                              0.36228209
## [3,]                                              0.25870995
## [4,]                                              0.25870995
## [5,]                                              0.25870995
## [6,]                                              0.05600958
```

???

- ...

---

# But What About Cross-Validation?

Almost forgot! CV is necessary in order to honestly evaluate our models and
avoid overfitting. Thus, we need to be able to easily cross-validate the whole
process. We provide facilities for easily doing this, based on the [`origami`
package](https://github.com/jeremyrcoyle/origami).

--

Cross-validating a stacked model is as easy as instantiating and training a new
`Lrnr_cv`. Let's do this with the stacked model we created before


```r
cv_stack &lt;- Lrnr_cv$new(learner_stack)
cv_fit &lt;- cv_stack$train(task)
```

---

# Putting it all together: Super Learning

We can build a Super Learner model by combining a cross-validated stacked model
with a meta-learning algorithm.

Let's do this with the cross-validated stack we just created, using a GLM as our
meta-learner:


```r
glm_stack &lt;- Pipeline$new(cv_stack, glm_learner)
```

--

Just as in the cases we've already seen, training the Super Learner is an
essentially trivial process:


```r
ml_fit &lt;- glm_stack$train(task)
```

--

What do the predictions from our new Super Learner look like?


```r
preds_sl &lt;- ml_fit$predict()
head(preds_sl)
```

```
##         1         2         3         4         5         6 
## 0.3129252 0.3078742 0.2756114 0.2715259 0.2756114 0.1358479
```

???

- Worth mentioning that the flexibility offered by our design allows us to
invoke the Super Learner algorithm, but we can also do a lot more...

---
class: center, middle

# Thanks!

We have a great team: Jeremy Coyle, Nima Hejazi, Ivana Malenica, Oleg Sofrygin.

Slides created via the R package
[**xaringan**](https://github.com/yihui/xaringan).

Powered by [remark.js](https://remarkjs.com),
[**knitr**](http://yihui.name/knitr), and
[R Markdown](https://rmarkdown.rstudio.com).
    </textarea>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "zenburn",
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {window.dispatchEvent(new Event('resize'));});
(function() {var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler"); if (!r) return; s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }"; d.head.appendChild(s);})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
